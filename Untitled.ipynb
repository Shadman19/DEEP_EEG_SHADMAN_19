{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b1a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3242b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d759fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2711274",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973acd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = \"C:/Users/user/Downloads/EEG-Motor-Imagery-Classification-CNNs-TensorFlow-master/Test_Raw_Data/First_Try_Model/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b6f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f61b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Training_data.csv', header=None)\n",
    "train_data = np.array(train_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a257e3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0060333 , -1.0003798 , -0.9936877 , ..., -0.9265462 ,\n",
       "        -0.9381889 , -0.9451802 ],\n",
       "       [-1.2065778 , -1.200839  , -1.1922537 , ..., -1.0278494 ,\n",
       "        -1.0487924 , -1.0670733 ],\n",
       "       [ 0.5741294 ,  0.6663968 ,  0.75685763, ..., -0.43629694,\n",
       "        -0.501556  , -0.56210124],\n",
       "       ...,\n",
       "       [ 1.7855802 ,  2.1232593 ,  2.4432309 , ..., -1.1011789 ,\n",
       "        -1.1125612 , -1.1208526 ],\n",
       "       [-0.69341487, -0.69405276, -0.6946852 , ..., -0.67570335,\n",
       "        -0.6761837 , -0.6775463 ],\n",
       "       [-0.30521932, -0.23822486, -0.17910576, ..., -0.78552926,\n",
       "        -0.8111468 , -0.8306976 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#koo\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb62700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Labels\n",
    "train_labels = pd.read_csv('Training_labels.csv', header=None)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60de0e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1],\n",
       "       [2],\n",
       "       ...,\n",
       "       [0],\n",
       "       [2],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "268cdb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 24343 into shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-a854de459431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#koo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 24343 into shape (1,4)"
     ]
    }
   ],
   "source": [
    "#koo\n",
    "train_labels = train_labels.reshape(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9dfa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#koo\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59288df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Testing Data\n",
    "test_data = pd.read_csv('Test_data.csv', header=None)\n",
    "test_data = np.array(test_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7bd8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv('Test_labels.csv', header=None)\n",
    "test_labels = np.array(test_labels)\n",
    "#koo test_labels = np.squeeze(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25c37484",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_batch = train_data.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e754d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weights\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ab40f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e6df8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Function of Summary\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eaddb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e904f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Placeholders\n",
    "with tf.name_scope(\"Input\"):\n",
    "    # x is the input feature data\n",
    "    with tf.name_scope(\"Input_Data\"):\n",
    "        x = tf.placeholder(tf.float32, [None, 640])\n",
    "\n",
    "    # y is the label related to the data\n",
    "    with tf.name_scope(\"Labels\"):\n",
    "        y = tf.placeholder(tf.float32, [None, 4]) # koo replace(original 4) it with y = tf.placeholder(tf.float32, [None, 4])\n",
    "\n",
    "    # Keep_Prob is the possibility that keep neural while using dropout\n",
    "    with tf.name_scope(\"Keep_Prob\"):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Reshape the input data into 2-dimensional\n",
    "    with tf.name_scope(\"Reshape_Data\"):\n",
    "        x_Reshape = tf.reshape(tensor=x, shape=[-1, 32, 20, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "846b992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Convolutional Layer\n",
    "with tf.name_scope('Convolutional_1'):\n",
    "    with tf.name_scope('W_conv1'):\n",
    "        W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "        # variable_summaries(W_conv1)\n",
    "\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_variable([32])\n",
    "        # variable_summaries(b_conv1)\n",
    "\n",
    "    with tf.name_scope('h_conv1'):\n",
    "        h_conv1 = tf.nn.conv2d(x_Reshape, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "        # variable_summaries(h_conv1)\n",
    "\n",
    "    with tf.name_scope('h_conv1_Acti'):\n",
    "        h_conv1_Acti = tf.nn.leaky_relu(h_conv1)\n",
    "        # variable_summaries(h_conv1_Acti)\n",
    "\n",
    "    with tf.name_scope('h_conv1_drop'):\n",
    "        h_conv1_drop = tf.nn.dropout(h_conv1_Acti, keep_prob, noise_shape=[tf.shape(h_conv1_Acti)[0], 1, 1, tf.shape(h_conv1_Acti)[3]])\n",
    "        # variable_summaries(h_conv1_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7e0bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Convolutional Layer\n",
    "with tf.name_scope('Convolutional_2'):\n",
    "    with tf.name_scope('W_conv2'):\n",
    "        W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "        # variable_summaries(W_conv2)\n",
    "\n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_variable([32])\n",
    "        # variable_summaries(b_conv2)\n",
    "\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2 = tf.nn.conv2d(h_conv1_drop, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n",
    "        # variable_summaries(h_conv2)\n",
    "\n",
    "    with tf.name_scope('h_conv2_BN'):\n",
    "        h_conv2_BN = tf.layers.batch_normalization(h_conv2, training=True)\n",
    "        # variable_summaries(h_conv2_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv2_Acti'):\n",
    "        h_conv2_Acti = tf.nn.leaky_relu(h_conv2_BN)\n",
    "        # variable_summaries(h_conv2_Acti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb700f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Convolutional Layer\n",
    "with tf.name_scope('Convolutional_3'):\n",
    "    with tf.name_scope('W_conv3'):\n",
    "        W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv3)\n",
    "\n",
    "    with tf.name_scope('b_conv3'):\n",
    "        b_conv3 = bias_variable([64])\n",
    "        # variable_summaries(b_conv3)\n",
    "\n",
    "    with tf.name_scope('h_conv3_res'):\n",
    "        h_conv3_res = tf.concat([h_conv2_Acti, h_conv1_drop], axis=3)\n",
    "        # variable_summaries(h_conv3_res)\n",
    "\n",
    "    with tf.name_scope('h_conv3'):\n",
    "        h_conv3 = tf.nn.conv2d(h_conv3_res, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3\n",
    "        # variable_summaries(h_conv3)\n",
    "\n",
    "    with tf.name_scope('h_conv3_Acti'):\n",
    "        h_conv3_Acti = tf.nn.leaky_relu(h_conv3)\n",
    "        # variable_summaries(h_conv3_Acti)\n",
    "\n",
    "    with tf.name_scope('h_pool3_drop'):\n",
    "        h_conv3_drop = tf.nn.dropout(h_conv3_Acti, keep_prob, noise_shape=[tf.shape(h_conv3_Acti)[0], 1, 1, tf.shape(h_conv3_Acti)[3]])\n",
    "        # variable_summaries(h_conv3_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17479124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Max Pooling Layer\n",
    "with tf.name_scope('Pooling_1'):\n",
    "    with tf.name_scope('h_pool3'):\n",
    "        h_pool3 = tf.nn.max_pool(h_conv3_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # variable_summaries(h_pool3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b794ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_4'):\n",
    "    with tf.name_scope('W_conv4'):\n",
    "        W_conv4 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv4)\n",
    "\n",
    "    with tf.name_scope('b_conv4'):\n",
    "        b_conv4 = bias_variable([64])\n",
    "        # variable_summaries(b_conv4)\n",
    "\n",
    "    with tf.name_scope('h_conv4'):\n",
    "        h_conv4 = tf.nn.conv2d(h_pool3, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4\n",
    "        # variable_summaries(h_conv4)\n",
    "\n",
    "    with tf.name_scope('h_conv4_BN'):\n",
    "        h_conv4_BN = tf.layers.batch_normalization(h_conv4, training=True)\n",
    "        # variable_summaries(h_conv4_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv4_Acti'):\n",
    "        h_conv4_Acti = tf.nn.leaky_relu(h_conv4_BN)\n",
    "        # variable_summaries(h_conv4_Acti)\n",
    "\n",
    "    with tf.name_scope('h_conv4_drop'):\n",
    "        h_conv4_drop = tf.nn.dropout(h_conv4_Acti, keep_prob, noise_shape=[tf.shape(h_conv4_Acti)[0], 1, 1, tf.shape(h_conv4_Acti)[3]])\n",
    "        # variable_summaries(h_conv4_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cfcdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_5'):\n",
    "    with tf.name_scope('W_conv5'):\n",
    "        W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv5)\n",
    "\n",
    "    with tf.name_scope('b_conv5'):\n",
    "        b_conv5 = bias_variable([64])\n",
    "        # variable_summaries(b_conv5)\n",
    "\n",
    "    with tf.name_scope('h_conv5'):\n",
    "        h_conv5 = tf.nn.conv2d(h_conv4_drop, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5\n",
    "        # variable_summaries(h_conv5)\n",
    "\n",
    "    with tf.name_scope('h_conv5_BN'):\n",
    "        h_conv5_BN = tf.layers.batch_normalization(h_conv5, training=True)\n",
    "        # variable_summaries(h_conv5_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv5_Acti'):\n",
    "        h_conv5_Acti = tf.nn.leaky_relu(h_conv5_BN)\n",
    "        # variable_summaries(h_conv5_Acti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d050813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_6'):\n",
    "    with tf.name_scope('W_conv6'):\n",
    "        W_conv6 = weight_variable([3, 3, 128, 128])\n",
    "        # variable_summaries(W_conv6)\n",
    "\n",
    "    with tf.name_scope('b_conv6'):\n",
    "        b_conv6 = bias_variable([128])\n",
    "        # variable_summaries(b_conv6)\n",
    "\n",
    "    with tf.name_scope('h_conv6_res'):\n",
    "        h_conv6_res = tf.concat([h_conv5_Acti, h_conv4_drop], axis=3)\n",
    "        # variable_summaries(h_conv6_res)\n",
    "\n",
    "    with tf.name_scope('h_conv6'):\n",
    "        h_conv6 = tf.nn.conv2d(h_conv6_res, W_conv6, strides=[1, 1, 1, 1], padding='SAME') + b_conv6\n",
    "        # variable_summaries(h_conv6)\n",
    "\n",
    "    with tf.name_scope('h_conv6_Activation'):\n",
    "        h_conv6_Acti = tf.nn.leaky_relu(h_conv6)\n",
    "        # variable_summaries(h_conv6_Acti)\n",
    "\n",
    "    with tf.name_scope('h_pool6_drop'):\n",
    "        h_conv6_drop = tf.nn.dropout(h_conv6_Acti, keep_prob, noise_shape=[tf.shape(h_conv6_Acti)[0], 1, 1, tf.shape(h_conv6_Acti)[3]])\n",
    "        # variable_summaries(h_conv6_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9af9c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Max Pooling Layer\n",
    "with tf.name_scope('Pooling_2'):\n",
    "    with tf.name_scope('h_pool6'):\n",
    "        h_pool6 = tf.nn.max_pool(h_conv6_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # variable_summaries(h_pool6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8a4cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "with tf.name_scope('Flatten'):\n",
    "    with tf.name_scope('h_pool6_flat'):\n",
    "        h_pool6_flat = tf.reshape(h_pool6, [-1, 4 * 7 * 128])\n",
    "        # variable_summaries(h_pool6_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "874b4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Fully Connected Layer\n",
    "with tf.name_scope('Fully_Connected_1'):\n",
    "    with tf.name_scope('W_fc1'):\n",
    "        W_fc1 = weight_variable([4 * 7 * 128, 512])\n",
    "        # variable_summaries(W_fc1)\n",
    "\n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc1 = bias_variable([512])\n",
    "        # variable_summaries(b_fc1)\n",
    "\n",
    "    with tf.name_scope('h_fc1'):\n",
    "        h_fc1 = tf.matmul(h_pool6_flat, W_fc1) + b_fc1\n",
    "        # variable_summaries(h_fc1)\n",
    "\n",
    "    with tf.name_scope('h_fc1_BN'):\n",
    "        h_fc1_BN = tf.layers.batch_normalization(h_fc1, training=True)\n",
    "        # variable_summaries(h_fc1_BN)\n",
    "\n",
    "    with tf.name_scope('h_fc1_Acti'):\n",
    "        h_fc1_Acti = tf.nn.leaky_relu(h_fc1_BN)\n",
    "        # variable_summaries(h_fc1_Acti)\n",
    "\n",
    "    with tf.name_scope('h_fc1_drop'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1_Acti, keep_prob)\n",
    "        # variable_summaries(h_fc1_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09562155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Fully Connected Layer\n",
    "with tf.name_scope('Output_Layer'):\n",
    "    with tf.name_scope('W_fc2'):\n",
    "        W_fc2 = weight_variable([512, 4])\n",
    "        # variable_summaries(W_fc2)\n",
    "\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc2 = bias_variable([4])\n",
    "        # variable_summaries(b_fc2)\n",
    "\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "        # variable_summaries(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72e7c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "with tf.name_scope('loss'):\n",
    "    with tf.name_scope('Euclidean_Distance'):\n",
    "        loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "        tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d81eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Optimizer\n",
    "with tf.name_scope('Train_Optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb1ef89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#koo\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "856a5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "# Add metrics to TensorBoard.\n",
    "with tf.name_scope('Evalution'):\n",
    "    # Calculate Each Task Accuracy\n",
    "    with tf.name_scope('Each_Class_accuracy'):\n",
    "        # Task 1 Accuracy\n",
    "        with tf.name_scope('T1_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T1 = tf.equal(tf.argmax(y, 1), 0)\n",
    "            prediction_T1 = tf.equal(tf.argmax(prediction, 1), 0)\n",
    "            T1_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T1), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T1_all_Num = tf.reduce_sum(tf.cast(y_T1, tf.float32))\n",
    "\n",
    "            # Task 1 Accuracy\n",
    "            T1_accuracy = tf.divide(T1_Corrected_Num, T1_all_Num)\n",
    "            tf.summary.scalar('T1_accuracy', T1_accuracy)\n",
    "\n",
    "            T1_TP = T1_Corrected_Num\n",
    "            T1_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T1), tf.math.logical_not(prediction_T1)), tf.float32))\n",
    "            T1_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T1), prediction_T1), tf.float32))\n",
    "            T1_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, tf.math.logical_not(prediction_T1)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T1_Precision\"):\n",
    "                T1_Precision = T1_TP / (T1_TP + T1_FP)\n",
    "                tf.summary.scalar('T1_Precision', T1_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T1_Recall\"):\n",
    "                T1_Recall = T1_TP / (T1_TP + T1_FN)\n",
    "                tf.summary.scalar('T1_Recall', T1_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T1_F_Score\"):\n",
    "                T1_F_Score = (2*T1_Precision*T1_Recall)/(T1_Precision+T1_Recall)\n",
    "                tf.summary.scalar('T1_F_Score', T1_F_Score)\n",
    "\n",
    "        # Task 2 Accuracy\n",
    "        with tf.name_scope('T2_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T2 = tf.equal(tf.argmax(y, 1), 1)\n",
    "            prediction_T2 = tf.equal(tf.argmax(prediction, 1), 1)\n",
    "            T2_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T2), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T2_all_Num = tf.reduce_sum(tf.cast(y_T2, tf.float32))\n",
    "\n",
    "            # Task 2 Accuracy\n",
    "            T2_accuracy = tf.divide(T2_Corrected_Num, T2_all_Num)\n",
    "            tf.summary.scalar('T2_accuracy', T2_accuracy)\n",
    "\n",
    "            T2_TP = T2_Corrected_Num\n",
    "            T2_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T2), tf.math.logical_not(prediction_T2)), tf.float32))\n",
    "            T2_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T2), prediction_T2), tf.float32))\n",
    "            T2_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, tf.math.logical_not(prediction_T2)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T2_Precision\"):\n",
    "                T2_Precision = T2_TP / (T2_TP + T2_FP)\n",
    "                tf.summary.scalar('T2_Precision', T2_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T2_Recall\"):\n",
    "                T2_Recall = T2_TP / (T2_TP + T2_FN)\n",
    "                tf.summary.scalar('T2_Recall', T2_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T2_F_Score\"):\n",
    "                T2_F_Score = (2*T2_Precision*T2_Recall)/(T2_Precision+T2_Recall)\n",
    "                tf.summary.scalar('T2_F_Score', T2_F_Score)\n",
    "\n",
    "        # Task 3 Accuracy\n",
    "        with tf.name_scope('T3_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T3 = tf.equal(tf.argmax(y, 1), 2)\n",
    "            prediction_T3 = tf.equal(tf.argmax(prediction, 1), 2)\n",
    "            T3_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T3), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T3_all_Num = tf.reduce_sum(tf.cast(y_T3, tf.float32))\n",
    "\n",
    "            # Task 3 Accuracy\n",
    "            T3_accuracy = tf.divide(T3_Corrected_Num, T3_all_Num)\n",
    "            tf.summary.scalar('T3_accuracy', T3_accuracy)\n",
    "\n",
    "            T3_TP = T3_Corrected_Num\n",
    "            T3_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T3), tf.math.logical_not(prediction_T3)), tf.float32))\n",
    "            T3_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T3), prediction_T3), tf.float32))\n",
    "            T3_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, tf.math.logical_not(prediction_T3)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T3_Precision\"):\n",
    "                T3_Precision = T3_TP / (T3_TP + T3_FP)\n",
    "                tf.summary.scalar('T3_Precision', T3_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T3_Recall\"):\n",
    "                T3_Recall = T3_TP / (T3_TP + T3_FN)\n",
    "                tf.summary.scalar('T3_Recall', T3_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T3_F_Score\"):\n",
    "                T3_F_Score = (2*T3_Precision*T3_Recall)/(T3_Precision+T3_Recall)\n",
    "                tf.summary.scalar('T3_F_Score', T3_F_Score)\n",
    "\n",
    "        # Task 4 Accuracy\n",
    "        with tf.name_scope('T4_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T4 = tf.equal(tf.argmax(y, 1), 3)\n",
    "            prediction_T4 = tf.equal(tf.argmax(prediction, 1), 3)\n",
    "            T4_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T4), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T4_all_Num = tf.reduce_sum(tf.cast(y_T4, tf.float32))\n",
    "\n",
    "            # Task 4 Accuracy\n",
    "            T4_accuracy = tf.divide(T4_Corrected_Num, T4_all_Num)\n",
    "            tf.summary.scalar('T4_accuracy', T4_accuracy)\n",
    "\n",
    "            T4_TP = T4_Corrected_Num\n",
    "            T4_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T4), tf.math.logical_not(prediction_T4)), tf.float32))\n",
    "            T4_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T4), prediction_T4), tf.float32))\n",
    "            T4_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, tf.math.logical_not(prediction_T4)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T4_Precision\"):\n",
    "                T4_Precision = T4_TP / (T4_TP + T4_FP)\n",
    "                tf.summary.scalar('T4_Precision', T4_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T4_Recall\"):\n",
    "                T4_Recall = T4_TP / (T4_TP + T4_FN)\n",
    "                tf.summary.scalar('T4_Recall', T4_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T4_F_Score\"):\n",
    "                T4_F_Score = (2*T4_Precision*T4_Recall)/(T4_Precision+T4_Recall)\n",
    "                tf.summary.scalar('T4_F_Score', T4_F_Score)\n",
    "\n",
    "    # Calculate the Confusion Matrix\n",
    "    with tf.name_scope(\"Confusion_Matrix\"):\n",
    "        with tf.name_scope(\"T1_Label\"):\n",
    "            T1_T1 = T1_Corrected_Num\n",
    "            T1_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T2), tf.float32))\n",
    "            T1_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T3), tf.float32))\n",
    "            T1_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T4), tf.float32))\n",
    "\n",
    "            T1_T1_percent = tf.divide(T1_T1, T1_all_Num)\n",
    "            T1_T2_percent = tf.divide(T1_T2, T1_all_Num)\n",
    "            T1_T3_percent = tf.divide(T1_T3, T1_all_Num)\n",
    "            T1_T4_percent = tf.divide(T1_T4, T1_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T1_T1_percent', T1_T1_percent)\n",
    "            tf.summary.scalar('T1_T2_percent', T1_T2_percent)\n",
    "            tf.summary.scalar('T1_T3_percent', T1_T3_percent)\n",
    "            tf.summary.scalar('T1_T4_percent', T1_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T2_Label\"):\n",
    "            T2_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T1), tf.float32))\n",
    "            T2_T2 = T2_Corrected_Num\n",
    "            T2_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T3), tf.float32))\n",
    "            T2_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T4), tf.float32))\n",
    "\n",
    "            T2_T1_percent = tf.divide(T2_T1, T2_all_Num)\n",
    "            T2_T2_percent = tf.divide(T2_T2, T2_all_Num)\n",
    "            T2_T3_percent = tf.divide(T2_T3, T2_all_Num)\n",
    "            T2_T4_percent = tf.divide(T2_T4, T2_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T2_T1_percent', T2_T1_percent)\n",
    "            tf.summary.scalar('T2_T2_percent', T2_T2_percent)\n",
    "            tf.summary.scalar('T2_T3_percent', T2_T3_percent)\n",
    "            tf.summary.scalar('T2_T4_percent', T2_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T3_Label\"):\n",
    "            T3_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T1), tf.float32))\n",
    "            T3_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T2), tf.float32))\n",
    "            T3_T3 = T3_Corrected_Num\n",
    "            T3_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T4), tf.float32))\n",
    "\n",
    "            T3_T1_percent = tf.divide(T3_T1, T3_all_Num)\n",
    "            T3_T2_percent = tf.divide(T3_T2, T3_all_Num)\n",
    "            T3_T3_percent = tf.divide(T3_T3, T3_all_Num)\n",
    "            T3_T4_percent = tf.divide(T3_T4, T3_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T3_T1_percent', T3_T1_percent)\n",
    "            tf.summary.scalar('T3_T2_percent', T3_T2_percent)\n",
    "            tf.summary.scalar('T3_T3_percent', T3_T3_percent)\n",
    "            tf.summary.scalar('T3_T4_percent', T3_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T4_Label\"):\n",
    "            T4_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T1), tf.float32))\n",
    "            T4_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T2), tf.float32))\n",
    "            T4_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T3), tf.float32))\n",
    "            T4_T4 = T4_Corrected_Num\n",
    "\n",
    "            T4_T1_percent = tf.divide(T4_T1, T4_all_Num)\n",
    "            T4_T2_percent = tf.divide(T4_T2, T4_all_Num)\n",
    "            T4_T3_percent = tf.divide(T4_T3, T4_all_Num)\n",
    "            T4_T4_percent = tf.divide(T4_T4, T4_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T4_T1_percent', T4_T1_percent)\n",
    "            tf.summary.scalar('T4_T2_percent', T4_T2_percent)\n",
    "            tf.summary.scalar('T4_T3_percent', T4_T3_percent)\n",
    "            tf.summary.scalar('T4_T4_percent', T4_T4_percent)\n",
    "\n",
    "    with tf.name_scope('Global_Evalution_Metrics'):\n",
    "        # Global Average Accuracy - Simple Algorithm\n",
    "        with tf.name_scope('Global_Average_Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            Global_Average_Accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Global_Average_Accuracy', Global_Average_Accuracy)\n",
    "\n",
    "        with tf.name_scope('Kappa_Metric'):\n",
    "            Test_Set_Num = T1_all_Num + T2_all_Num + T3_all_Num + T4_all_Num\n",
    "\n",
    "            Actual_T1 = T1_all_Num\n",
    "            Actual_T2 = T2_all_Num\n",
    "            Actual_T3 = T3_all_Num\n",
    "            Actual_T4 = T4_all_Num\n",
    "\n",
    "            Prediction_T1 = T1_T1 + T2_T1 + T3_T1 + T4_T1\n",
    "            Prediction_T2 = T1_T2 + T2_T2 + T3_T2 + T4_T2\n",
    "            Prediction_T3 = T1_T3 + T2_T3 + T3_T3 + T4_T3\n",
    "            Prediction_T4 = T1_T4 + T2_T4 + T3_T4 + T4_T4\n",
    "\n",
    "            p0 = (T1_T1 + T2_T2 + T3_T3 + T4_T4) / Test_Set_Num\n",
    "            pe = (Actual_T1*Prediction_T1 + Actual_T2*Prediction_T2 + Actual_T3*Prediction_T3 + Actual_T4*Prediction_T4) / \\\n",
    "                 (Test_Set_Num*Test_Set_Num)\n",
    "\n",
    "            Kappa_Metric = (p0 - pe) / (1 - pe)\n",
    "            tf.summary.scalar('Kappa_Metric', Kappa_Metric)\n",
    "\n",
    "        with tf.name_scope('Micro_Averaged_Evalution'):\n",
    "            with tf.name_scope(\"Micro_Averaged_Confusion_Matrix\"):\n",
    "                TP_all = T1_TP + T2_TP + T3_TP + T4_TP\n",
    "                TN_all = T1_TN + T2_TN + T3_TN + T4_TN\n",
    "                FP_all = T1_FP + T2_FP + T3_FP + T4_FP\n",
    "                FN_all = T1_FN + T2_FN + T3_FN + T4_FN\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_Precision\"):\n",
    "                Micro_Global_Precision = TP_all / (TP_all + FP_all)\n",
    "                tf.summary.scalar('Micro_Global_Precision', Micro_Global_Precision)\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_Recall\"):\n",
    "                Micro_Global_Recall = TP_all / (TP_all + FN_all)\n",
    "                tf.summary.scalar('Micro_Global_Recall', Micro_Global_Recall)\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_F1_Score\"):\n",
    "                Micro_Global_F1_Score = (2*Micro_Global_Precision*Micro_Global_Recall)/(Micro_Global_Precision+Micro_Global_Recall)\n",
    "                tf.summary.scalar('Micro_Global_F1_Score', Micro_Global_F1_Score)\n",
    "\n",
    "        with tf.name_scope('Macro_Averaged_Evalution'):\n",
    "            with tf.name_scope(\"Macro_Global_Precision\"):\n",
    "                Macro_Global_Precision = (T1_Precision + T2_Precision + T3_Precision + T4_Precision) / 4\n",
    "                tf.summary.scalar('Macro_Global_Precision', Macro_Global_Precision)\n",
    "\n",
    "            with tf.name_scope(\"Macro_Global_Recall\"):\n",
    "                Macro_Global_Recall = (T1_Recall + T2_Recall + T3_Recall + T4_Recall) / 4\n",
    "                tf.summary.scalar('Macro_Global_Recall', Macro_Global_Recall)\n",
    "\n",
    "            with tf.name_scope(\"Macro_Global_F1_Score\"):\n",
    "                Macro_Global_F1_Score = (T1_F_Score + T2_F_Score + T3_F_Score + T4_F_Score) / 4\n",
    "                tf.summary.scalar('Macro_Global_F1_Score', Macro_Global_F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eeb774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f16a2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all the variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9a46cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a saver to save the trained model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babe02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary the Training and Test Processing\n",
    "train_writer = tf.summary.FileWriter(SAVE + 'train_Writer', sess.graph)\n",
    "test_writer  = tf.summary.FileWriter(SAVE + 'test_Writer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "750fa346",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 1) for Tensor 'Input/Labels/Placeholder:0', which has shape '(None, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-7134c6f12961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.50\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Accuracy on Training Set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1152\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m   1153\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[1;32m-> 1154\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 1) for Tensor 'Input/Labels/Placeholder:0', which has shape '(None, 4)'"
     ]
    }
   ],
   "source": [
    "for epoch in range(2019):\n",
    "    for batch_index in range(n_batch):\n",
    "        random_batch = random.sample(range(train_data.shape[0]), batch_size)\n",
    "        batch_xs = train_data[random_batch]\n",
    "        batch_ys = train_labels[random_batch]\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.50})\n",
    "\n",
    "    # Accuracy on Training Set\n",
    "    train_acc, train_loss = sess.run([Global_Average_Accuracy, loss], feed_dict={x: train_data, y: train_labels, keep_prob: 1.0})\n",
    "\n",
    "    # Accuracy on Test Set\n",
    "    test_summary, test_acc, test_loss = sess.run([merged, Global_Average_Accuracy, loss], feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "    test_writer.add_summary(test_summary, epoch)\n",
    "    \n",
    "    # Show the Model Capability\n",
    "    print(\"Iter \" + str(epoch) + \", Training Accuracy: \" + str(train_acc) + \", Testing Accuracy: \" + str(test_acc))\n",
    "\n",
    "    # Save the Model Every 100 Epoches\n",
    "    if epoch % 100 == 0:\n",
    "        saver.save(sess, save_path=SAVE + 'Model_Saver/Ite_%s' % epoch)\n",
    "\n",
    "    if epoch == 2001:\n",
    "        output_prediction = sess.run(prediction, feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "        np.savetxt(SAVE + \"prediction.csv\", output_prediction, delimiter=\",\")\n",
    "        np.savetxt(SAVE + \"labels.csv\", test_labels, delimiter=\",\")\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba4e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
